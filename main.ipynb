{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bio_data_path =  ./data/snips_AddToPlaylist_BO/test_bbi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyn2020/anaconda3/envs/allennlp0.9/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
      "Your BERT model appears to be uncased, but your indexer is not lowercasing tokens.\n"
     ]
    }
   ],
   "source": [
    "from types import new_class\n",
    "from allennlp.models import model\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.common import Params\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data import DataIterator#\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.dataset_readers.dataset_utils.span_utils import bio_tags_to_spans\n",
    "from allennlp.models import Model\n",
    "from allennlp.training import Trainer,checkpointer\n",
    "from allennlp.training.util import evaluate\n",
    "from allennlp.common.util import prepare_global_logging, cleanup_global_logging, prepare_environment\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, PretrainedBertIndexer\n",
    "from allennlp.data import vocabulary\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "from models import NSDSlotTaggingModel\n",
    "from predictors import SlotFillingPredictor\n",
    "from dataset_readers import MultiFileDatasetReader\n",
    "from metrics import NSDSpanBasedF1Measure\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Any, Union, Dict, Iterable, List, Optional, Tuple\n",
    "from time import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "vocabulary.DEFAULT_OOV_TOKEN = \"[UNK]\"  # set for bert\n",
    "\n",
    "output_dir = \"./output/snips_BO\"\n",
    "dataset = \"snips_AddToPlaylist_BO\"\n",
    "cuda = 0\n",
    "threshold = \"auto\"\n",
    "batch_size = 256\n",
    "\n",
    "# Test\n",
    "time_begin = time()\n",
    "model_dir = os.path.join(output_dir,dataset)\n",
    "\n",
    "params = Params.from_file(os.path.join(model_dir,\"config.json\"))\n",
    "vocab = Vocabulary.from_files(os.path.join(model_dir,\"vocabulary\"))\n",
    "test_bio_data_path = params.pop(\"test_data_path_bio\", None)\n",
    "test_data_path = params.pop(\"test_data_path\", None)\n",
    "train_data_path = params.pop(\"train_data_path\", None)\n",
    "print(\"test_bio_data_path = \",test_bio_data_path)\n",
    "\n",
    "# predict\n",
    "archive = load_archive(model_dir,cuda_device=cuda)\n",
    "predictor = Predictor.from_archive(archive=archive, predictor_name=\"slot_filling_predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyn2020/anaconda3/envs/allennlp0.9/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
      "Your BERT model appears to be uncased, but your indexer is not lowercasing tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: predict_labels = 1177, true_labels = 1177, encoder_outs = (1177, 256), tokens = 124\n",
      "output: predict_labels = 1177, true_labels = 1177, encoder_outs = (1177, 256), tokens = 124\n"
     ]
    }
   ],
   "source": [
    "time_begin = time()\n",
    "model_dir = os.path.join(output_dir,dataset)\n",
    "\n",
    "# predict\n",
    "archive = load_archive(model_dir,cuda_device=cuda)\n",
    "predictor = Predictor.from_archive(archive=archive, predictor_name=\"slot_filling_predictor\")\n",
    "# train_outputs = predictor.predict_multi(file_path = train_data_path ,batch_size = batch_size)\n",
    "test_outputs = predictor.predict_multi(file_path = test_data_path ,batch_size = batch_size)\n",
    "test_bbi_outputs = predictor.predict_multi(file_path = test_bio_data_path ,batch_size = batch_size)  # bert输出\n",
    "\n",
    "# input_text = \"add sabrina salerno salerno to the grime instrumentals playlist add sabrina salerno salerno to the grime instrumentals playlist\" # TODO 用于Debug的样例\n",
    "# input_label = ['O', 'B-artist', 'I-artist', 'B-artist', 'O', 'O', 'B-playlist', 'I-playlist', 'B-playlist','B-playlist', 'B-artist', 'I-artist', 'B-artist', 'O', 'O', 'B-playlist', 'I-playlist', 'O'] # TODO 用于Debug的样例\n",
    "# input_bi_label = ['O', 'B-entity', 'B-entity', 'B-entity', 'O', 'O', 'B-entity', 'B-entity', 'B-entity', 'B-entity', 'B-entity', 'B-entity', 'B-entity', 'O', 'O', 'B-entity', 'B-entity', 'O']\n",
    "# print(input_label)\n",
    "# print(input_bi_label)\n",
    "# test_bbi_outputs = predictor.predict({\"tokens\": input_text.split(),\"true_labels\":input_bi_label}) # TODO 用于Debug的样例\n",
    "# test_outputs = predictor.predict({\"tokens\": input_text.split(),\"true_labels\":input_label}) # TODO 用于Debug的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————第一步：（实体识别）计算在目标领域测试集上实体token的识别指标——————————————————————————\n",
      "- 测试集示例: \n",
      "- Ground Truth：  ['O', 'B-artist', 'I-artist', 'O', 'O', 'B-playlist', 'I-playlist', 'O', 'O', 'B-artist']\n",
      "- Predict Labels:  ['O', 'B-entity', 'B-entity', 'O', 'O', 'B-entity', 'B-entity', 'O', 'O', 'B-entity']\n",
      "\n",
      "- 在测试集上的eneity指标(token-metrics):  f:90.02, p:96.49, r:84.36\n",
      "------------------------------------------------------------------------------------------\n",
      "———————————————————————— 第二步：（聚类）计算聚类指标 ——————————————————————————\n",
      "vob_clusters_label2id =  {'artist': 0, 'playlist_owner': 1, 'O': 2, 'playlist': 3, 'entity_name': 4, 'music_item': 5}\n",
      "__________________________________________________________________________________\n",
      "model\t\tACC\tMI\tRI\tAMI\tARI\thomo\tcomp\tvmeas\n",
      "kmenas   \t0.560\t0.657\t0.719\t0.447\t0.320\t0.494\t0.420\t0.454\n",
      "__________________________________________________________________________________\n",
      "混淆矩阵的标签 =  ['artist', 'playlist_owner', 'O', 'playlist', 'entity_name', 'music_item']\n",
      "混淆矩阵：\n",
      " [[ 95   0   6   3   5   1]\n",
      " [  6   0   0   0   6   0]\n",
      " [  0   0   0   2  19   0]\n",
      " [ 18   0 119 138  44   1]\n",
      " [ 18   0  10   5  48   0]\n",
      " [  0   0   0   0   0  54]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        artist       0.69      0.86      0.77       110\n",
      "playlist_owner       0.00      0.00      0.00        12\n",
      "             O       0.00      0.00      0.00        21\n",
      "      playlist       0.93      0.43      0.59       320\n",
      "   entity_name       0.39      0.59      0.47        81\n",
      "    music_item       0.96      1.00      0.98        54\n",
      "\n",
      "      accuracy                           0.56       598\n",
      "     macro avg       0.50      0.48      0.47       598\n",
      "  weighted avg       0.77      0.56      0.61       598\n",
      "\n",
      "————————--——————————————————————— 第三步：（规则聚类反馈BIO） ——————————————————————————————————————\n",
      "seq_lens =  [9, 9]\n",
      "0              O\n",
      "1         artist\n",
      "2         artist\n",
      "3              O\n",
      "4              O\n",
      "          ...   \n",
      "1172           O\n",
      "1173    playlist\n",
      "1174    playlist\n",
      "1175    playlist\n",
      "1176           O\n",
      "Length: 1177, dtype: object\n",
      "['O', 'artist', 'artist', 'O', 'O', 'playlist', 'playlist', 'O', 'O']\n",
      "['artist', 'artist', 'O', 'entity_name', 'O', 'O', 'entity_name', 'O', 'O']\n",
      "final_pred_labels =  18 ['O', 'B-artist', 'I-artist', 'O', 'O', 'B-playlist', 'I-playlist', 'O', 'O', 'B-artist', 'I-artist', 'O', 'B-entity_name', 'O', 'O', 'B-entity_name', 'O', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wyn2020/anaconda3/envs/allennlp0.9/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data1/wyn2020/anaconda3/envs/allennlp0.9/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data1/wyn2020/anaconda3/envs/allennlp0.9/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 为计算后边指标，扩展vocab\n",
    "vocab_labels = list(vocab.get_index_to_token_vocabulary(\"labels\").values())\n",
    "pred_vocab_labels = list(set(test_outputs[\"true_labels\"]+test_outputs[\"predict_labels\"]))\n",
    "for pred_label in pred_vocab_labels:\n",
    "    if pred_label not in vocab_labels: vocab.add_token_to_namespace(pred_label,namespace = \"labels\") \n",
    "\n",
    "print(\"————————————————————————第一步：（实体识别）计算在目标领域测试集上实体token的识别指标——————————————————————————\")\n",
    "print(\"- 测试集示例: \")\n",
    "print(\"- Ground Truth： \",test_outputs[\"true_labels\"][:10])\n",
    "print(\"- Predict Labels: \",test_bbi_outputs[\"predict_labels\"][:10])\n",
    "entity_true_idx_tens = torch.Tensor([[vocab.get_token_index(label, namespace=\"labels\") for label in test_bbi_outputs[\"true_labels\"]]])\n",
    "pred_idx = [vocab.get_token_index(label, namespace=\"labels\") for label in test_bbi_outputs[\"predict_labels\"]]  \n",
    "pred_idx_tens = np.zeros((1,len(test_bbi_outputs[\"predict_labels\"]), vocab.get_vocab_size(\"labels\")),np.int)\n",
    "for token_ in range(pred_idx_tens.shape[1]):\n",
    "    pred_idx_tens[0,token_,pred_idx[token_]] = 1  \n",
    "pred_idx_tens = torch.Tensor(pred_idx_tens)\n",
    "spanf1 = NSDSpanBasedF1Measure(\n",
    "                                vocabulary = vocab,\n",
    "                                tag_namespace=\"labels\",\n",
    "                                ignore_classes=[],\n",
    "                                label_encoding=\"BIO\",\n",
    "                                nsd_slots=[\"ns\"]\n",
    "                                )\n",
    "spanf1(pred_idx_tens,entity_true_idx_tens) # pred_idx_tens(1,n_samples,n_classes)，true_idx_tens(1,n_samples)\n",
    "metric = spanf1.get_metric(reset=True)\n",
    "f,r,p = round(metric[\"f1-entity\"]*100,2),round(metric[\"recall-entity\"]*100,2),round(metric[\"precision-entity\"]*100,2)\n",
    "print(f\"\\n- 在测试集上的eneity指标(token-metrics):  f:{f}, p:{p}, r:{r}\")\n",
    "print(\"-\"*90)\n",
    "# TODO 可以看一下BIO形式更好还是BO形式更好\n",
    "\n",
    "print(\"———————————————————————— 第二步：（聚类）计算聚类指标 ——————————————————————————\")\n",
    "# 提取实体token embedding，以及对应的标注(为了对齐聚类指标以及对聚类效果进行评估)；\n",
    "true_labels_nobi = [_label if _label==\"O\" else _label[2:] for _label in test_outputs[\"true_labels\"]]\n",
    "pred_labels_nobi = [_label if _label==\"O\" else _label[2:] for _label in test_outputs[\"predict_labels\"]]\n",
    "true_labels_pd = pd.Series(true_labels_nobi)\n",
    "pred_labels_pd = pd.Series(pred_labels_nobi)\n",
    "pred_entity_idx = pred_labels_pd[pred_labels_pd.isin([\"entity\"])].index\n",
    "# print(\"pred_entity_idx = \",pred_entity_idx)\n",
    "# print(\"true_labels_pd = \\n\",true_labels_pd)\n",
    "# print(\"pred_labels_pd = \\n\",pred_labels_pd)\n",
    "cluster_true_labels = true_labels_pd[pred_entity_idx]\n",
    "cluster_true_labels = cluster_true_labels.tolist()\n",
    "token_embeddings = test_outputs[\"encoder_outs\"][pred_entity_idx]\n",
    "# slots\n",
    "slots = list(set(true_labels_nobi))\n",
    "slots.remove(\"O\")\n",
    "slots_o = list(set(true_labels_nobi))\n",
    "\n",
    "# 构建 slot_o 与 id 之间的映射关系\n",
    "vob_clusters_label2id = {}\n",
    "vob_clusters_id2label = {}\n",
    "for i, item in enumerate(slots_o):\n",
    "    vob_clusters_label2id[item] = i\n",
    "    vob_clusters_id2label[i] = item\n",
    "print(\"vob_clusters_label2id = \",vob_clusters_label2id)\n",
    "gold_cluster_labels = np.array([vob_clusters_label2id[label] for label in cluster_true_labels])\n",
    "\n",
    "# 将span embedding 输入给kmeans，判断类别；类别与原类别对齐，计算指标.\n",
    "cluster_contain_o = True\n",
    "if cluster_contain_o: \n",
    "    n_cluster = len(slots_o)\n",
    "    clusters = slots_o\n",
    "else:\n",
    "    n_cluster = len(slots)\n",
    "    clusters = slots\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=len(slots), n_init=10, random_state=0) # TODO : 算\"O\"，则n_cluster = len(slots)+1; 否则 n_cluster = len(slots)+1\n",
    "cluster_pred_labels = kmeans.fit_predict(token_embeddings)\n",
    "cluster_to_true,cluster_pred_labels = hungray_aligment(y_true=gold_cluster_labels,y_pred=cluster_pred_labels) \n",
    "kmeans_metrics(gold_cluster_labels,cluster_pred_labels) # gold_cluster_labels(n_samples),span_embeddings(n_samples, n_features)\n",
    "\n",
    "cluster_pred_labels = [vob_clusters_id2label[id] for id in cluster_pred_labels]\n",
    "print(\"混淆矩阵的标签 = \",clusters)\n",
    "cm = confusion_matrix(cluster_true_labels,cluster_pred_labels,labels=clusters)\n",
    "report = classification_report(cluster_true_labels,cluster_pred_labels,labels=clusters)\n",
    "print(\"混淆矩阵：\\n\",cm)\n",
    "print(report)\n",
    "\n",
    "print(\"————————--——————————————————————— 第三步：（规则聚类反馈BIO） ——————————————————————————————————————\")\n",
    "# 先将聚类结果反馈回第一步预测的实体序列\n",
    "n = 0 \n",
    "for entity_idx in pred_entity_idx:\n",
    "    pred_labels_pd[entity_idx] = cluster_pred_labels[n]\n",
    "    n += 1\n",
    "# 再利用规则改成BIO形式\n",
    "# def load_seq_len(data_dir:str):\n",
    "#     slot_label_file = os.path.join(data_dir, \"seq.out\")\n",
    "#     with open(slot_label_file, \"r\") as f:\n",
    "#         out_lines = f.readlines()\n",
    "#     seq_len = [len(out_line.strip().split(\" \"))\n",
    "#                     for out_line in out_lines if out_line.strip()]\n",
    "#     return seq_len\n",
    "# seq_lens = load_seq_len(data_dir = test_data_path) # list，数据集每个sentence的长度\n",
    "seq_lens = [9,9]\n",
    "print(\"seq_lens = \",seq_lens)\n",
    "final_pred_labels = []\n",
    "start_idx,end_idx = 0,0\n",
    "print(pred_labels_pd)\n",
    "for seq_len in seq_lens: # 注意一下分句的情况(上句结尾为某tag，下句开头同为某tag)\n",
    "    start_idx = end_idx # 第x条句子的起始索引\n",
    "    end_idx = start_idx + seq_len # 第x条句子的终止索引\n",
    "    seq_pred_labels = pred_labels_pd[start_idx:end_idx].tolist()\n",
    "    print(seq_pred_labels)\n",
    "    seq_deque = collections.deque(seq_pred_labels)\n",
    "    last_item = seq_deque.popleft()\n",
    "    final_pred_labels.append(\"O\" if last_item==\"O\" else \"B-\"+last_item)\n",
    "    for i in range(len(seq_deque)):\n",
    "        item = seq_deque.popleft()\n",
    "        if item == last_item:\n",
    "            final_pred_labels.append(\"O\" if item==\"O\" else \"I-\"+item)\n",
    "        else:\n",
    "            final_pred_labels.append(\"O\" if item==\"O\" else \"B-\"+item)\n",
    "        last_item = item\n",
    "print(\"final_pred_labels = \",len(final_pred_labels),final_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————--———————————————————————————— (Plot PCA) —————————————--———————————————————————\n",
      "0           O\n",
      "1      artist\n",
      "2      artist\n",
      "3           O\n",
      "4           O\n",
      "5    playlist\n",
      "6    playlist\n",
      "7           O\n",
      "8           O\n",
      "9      artist\n",
      "dtype: object\n",
      "0               O\n",
      "1          artist\n",
      "2          artist\n",
      "3               O\n",
      "4               O\n",
      "5        playlist\n",
      "6        playlist\n",
      "7               O\n",
      "8               O\n",
      "9          artist\n",
      "10         artist\n",
      "11              O\n",
      "12    entity_name\n",
      "13              O\n",
      "14              O\n",
      "15    entity_name\n",
      "16              O\n",
      "17              O\n",
      "dtype: object 18\n",
      "-----------------------\n",
      "(1177, 256)\n",
      "features.shape =  (1177, 2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1177 but corresponding boolean dimension is 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d3b66b39c9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_slot_o\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_slot_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mpca_2D_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_slot_o\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./error_study\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_2D_pred.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mpca_3D_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_slot_o\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./error_study\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_3D_pred.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-d3b66b39c9ed>\u001b[0m in \u001b[0;36mpca_2D_visualization\u001b[0;34m(X, y, classes, save_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m                     label=_class, alpha=0.5, s=10, edgecolors='none', color=\"gray\")\n\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             ax.scatter(red_features[y == _class, 0], red_features[y == _class, 1],\n\u001b[0m\u001b[1;32m     58\u001b[0m                     label=_class, alpha=0.5, s=10, edgecolors='none', zorder=15)\n\u001b[1;32m     59\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1177 but corresponding boolean dimension is 18"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def pca_3D_visualization(X: np.ndarray,\n",
    "                      y: pd.Series,\n",
    "                      classes: List[str],\n",
    "                      save_path: str):\n",
    "    \"\"\"\n",
    "    Apply PCA visualization for features.\n",
    "    \"\"\"\n",
    "    print(\"-----------------------\")\n",
    "    print(X.shape)\n",
    "    red_features = PCA(n_components=3, svd_solver=\"full\").fit_transform(X)\n",
    "#     print(red_features[:2])\n",
    "    print(\"features.shape = \",red_features.shape)\n",
    "\n",
    "    plt.style.use(\"seaborn-darkgrid\")\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = Axes3D(fig)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y', rotation=38)  # y 轴名称旋转 38 度\n",
    "    ax.set_zlabel('Z')  # 因为 plt 不能设置 z 轴坐标轴名称，所以这里只能用 ax 轴来设置（当然，x 轴和 y 轴的坐标轴名称也可以用 ax 设置）\n",
    "\n",
    "    for _class in classes:\n",
    "        if _class == \"O\":\n",
    "            ax.scatter(red_features[y == _class, 0], red_features[y == _class, 1], red_features[y == _class, 2],\n",
    "                    label=_class, alpha=0.5, s=10, edgecolors='none', color=\"gray\")\n",
    "        else:\n",
    "            ax.scatter(red_features[y == _class, 0], red_features[y == _class, 1], red_features[y == _class, 2],\n",
    "                    label=_class, alpha=0.5, s=10, edgecolors='none', zorder=15)\n",
    "    ax.legend(loc=2)\n",
    "    ax.grid(True)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0, format=\"pdf\")\n",
    "#     plt.savefig(save_path, format=\"png\")\n",
    "def pca_2D_visualization(X: np.ndarray,\n",
    "                      y: pd.Series,\n",
    "                      classes: List[str],\n",
    "                      save_path: str):\n",
    "    \"\"\"\n",
    "    Apply PCA visualization for features.\n",
    "    \"\"\"\n",
    "    print(\"-----------------------\")\n",
    "    print(X.shape)\n",
    "    red_features = PCA(n_components=2, svd_solver=\"full\").fit_transform(X)\n",
    "    print(\"features.shape = \",red_features.shape)\n",
    "\n",
    "    plt.style.use(\"seaborn-darkgrid\")\n",
    "#     fig,ax = plt.figure(figsize=(10,10))\n",
    "    fig, ax = plt.subplots()\n",
    "    for _class in classes:\n",
    "        if _class == \"O\":\n",
    "            ax.scatter(red_features[y == _class, 0], red_features[y == _class, 1], \n",
    "                    label=_class, alpha=0.5, s=10, edgecolors='none', color=\"gray\")\n",
    "        else:\n",
    "            ax.scatter(red_features[y == _class, 0], red_features[y == _class, 1],\n",
    "                    label=_class, alpha=0.5, s=10, edgecolors='none', zorder=15)\n",
    "    ax.legend(loc=2)\n",
    "    ax.grid(True)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0, format=\"pdf\")\n",
    "#     plt.savefig(save_path, format=\"png\")\n",
    "print(\"————————--———————————————————————————— (Plot PCA) —————————————--———————————————————————\")\n",
    "true_slot_o = pd.Series([label if label == \"O\" else label[2:] for label in test_outputs[\"true_labels\"]])\n",
    "pred_slot_o = pd.Series([label if label == \"O\" else label[2:] for label in final_pred_labels])\n",
    "print(true_slot_o[:10])\n",
    "print(pred_slot_o,len(pred_slot_o))\n",
    "\n",
    "pca_2D_visualization(test_outputs[\"encoder_outs\"],pred_slot_o,clusters,os.path.join(\"./error_study\",dataset+\"_2D_pred.pdf\"))\n",
    "pca_3D_visualization(test_outputs[\"encoder_outs\"],pred_slot_o,clusters,os.path.join(\"./error_study\",dataset+\"_3D_pred.pdf\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1032983e2a5d0a88bd27d63ab918f5aa71c03f1dceec5ba08be88008b41ae7e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('allennlp0.9': conda)",
   "language": "python",
   "name": "python365jvsc74a57bd0f1032983e2a5d0a88bd27d63ab918f5aa71c03f1dceec5ba08be88008b41ae7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
